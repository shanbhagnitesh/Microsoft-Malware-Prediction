##############################
#  4 - U-bagging Modelling   #
##############################

# Load-in the libraries required
install.packages("caret")
library(caret)
library(caTools)
library(e1071)
library(ROCR)
library(dplyr)

# create_output_file_path - Function to create output path
create_output_file_path <- function(cv_id,cvrun_id,under_rate){
  filetrain1PathPred <-
    paste0("C:/Users/nshanbhag/Desktop/BigDataAnalytics/Machine learning for Marketing/Group Project/output_predicted/",
           "unb2_cv",cv_id,
           "train1",cvrun_id,
           "under",under_rate,"_p.csv")
  
  filetest1PathPred <-
    paste0("C:/Users/nshanbhag/Desktop/BigDataAnalytics/Machine learning for Marketing/Group Project/output_predicted/",
           "unb2_cv",cv_id,
           "test1",cvrun_id,
           "under",under_rate,"_p.csv")
  
  filePathAUC <-
    paste0("C:/Users/nshanbhag/Desktop/BigDataAnalytics/Machine learning for Marketing/Group Project/output_auc/",
           "unb2_cv",cv_id,
           "run",cvrun_id,
           "under",under_rate,"_AUC.csv")
  
  filePath <- data.frame("filetrain1PathPred" = filetrain1PathPred,
                         "filetest1PathPred" = filetest1PathPred,
                         "filePathAUC"=filePathAUC)
  filePath$filetrain1PathPred <- as.character(filePath$filetrain1PathPred)
  filePath$filetest1PathPred <- as.character(filePath$filetest1PathPred)
  filePath$filePathAUC <- as.character(filePath$filePathAUC)
  return(filePath)
}

# random_under_sample_list_generator - Function to create random under sample list
random_under_sample_list_generator <- function(iters, dsn_train1_0, dsn_train1_1, under){
  
  if(nrow(dsn_train1_0) > under){
    under_sample <- rbind(dsn_train1_1,dsn_train1_0[sample(1:nrow(dsn_train1_0),under,replace=F),])
  } else{
    under_sample <- rbind(dsn_train1_1,dsn_train1_0[sample(1:nrow(dsn_train1_0),under,replace=T),])
  }
  return(under_sample)
}

# calculate_auc - Function to calculate AUC
calculate_auc <- function(under_obs, test1_p){
  
  pred <- prediction(test1_p[,under_obs],test1_p[,"HasDetections"])
  auc<- performance(pred,"auc")
  auc <- unlist(slot(auc, "y.values"))
  
  return(auc)
}

# calculate_performance_params - Function to calculate Accuracy, Sensitivity, Specficity,
# Top10Lift, Top20Lift, Top30Lift, Top40Lift, Top50Lift
calculate_performance_params <- function(under_obs, test1_p){
  
  predicted <- as.numeric(test1_p[,under_obs]>0.5)
  reference <- test1_p[,"HasDetections"]
  u <- union(predicted, reference)
  xtab <- table(factor(predicted, u), factor(reference, u))
  acc <- confusionMatrix(xtab)$overall[[1]]
  Sensitivity <- confusionMatrix(xtab)$byClass[[1]]
  Specificity <- confusionMatrix(xtab)$byClass[[2]]
  
  predicted <- test1_p[,under_obs]
  
  Top10Lift <- calculate_lift(predicted, reference, 10)
  Top20Lift <- calculate_lift(predicted, reference, 20)
  Top30Lift <- calculate_lift(predicted, reference, 30)
  Top40Lift <- calculate_lift(predicted, reference, 40)
  Top50Lift <- calculate_lift(predicted, reference, 50)
  
  df <- data.frame("acc" = acc, "Sensitivity" = Sensitivity, "Specificity"= Specificity, 
                   "Top10Lift" = Top10Lift, "Top20Lift" = Top20Lift,
                   "Top30Lift" = Top30Lift, "Top40Lift" = Top40Lift,
                   "Top50Lift" = Top50Lift)
  
  return(df)
}

# calculate_lift- For calculating Lift
calculate_lift <- function(predicted, reference, val){
  
  if (is.factor(reference)) 
    reference <- as.integer(as.character(reference))
  
  lift_val <- data.frame(predicted, reference)
  lift_val <- lift_val[order(-lift_val[, 1]), ]
  lift_val <- lift_val[1:floor(nrow(lift_val)*val/100),]
  lift_val$predicted <- ifelse(lift_val$predicted > 0.5,1,0)
  lift_val$result <- ifelse(lift_val$predicted == lift_val$reference & lift_val$predicted == 1, 1,0)
  
  res <- as.numeric(mean(lift_val$result)/mean(reference))
  return(res)
}

# 5 * 2 folds cross validation
for(cv_id in c(1:5)){
  for(cvrun_id in c(1:2)){
    for(under_rate in seq(0.4,0.5, by = 0.1)){
      
      print(paste0("under_rate",under_rate,"_cv_id",cv_id,"_cvrun_id",cvrun_id))
      start_time <- Sys.time()
      
      # Create a vector with only required variables
      unwanted_cols <- c("customerID")
      wanted_cols <- names(train1)[!names(train1) %in% unwanted_cols]
      
      # Subset the data on required variables
      dsn_train1 <- train1[,wanted_cols]
      dsn_test1 <- test1[,wanted_cols]
      
      # Subset the data on HasDetections 0 or 1
      dsn_train1_0 <- subset(dsn_train1, HasDetections == 0)
      dsn_train1_1 <- subset(dsn_train1, HasDetections == 1)
      
      # Calculate the undersampling rate
      under <- round(nrow(dsn_train1_1)/(1-under_rate) - nrow(dsn_train1_1))
      
      # Create a final data frame based on under-sampling rate
      dsn_train1_f <- lapply(1:20,random_under_sample_list_generator, dsn_train1_0, dsn_train1_1, under)
      
      # Perform normal Logistic regression
      mylogit <- glm(HasDetections ~ ., data = dsn_train1, family = binomial)
      
      dsn_test1_p <- data.frame(HasDetections= dsn_test1$HasDetections)
      dsn_train1_p <- data.frame(HasDetections= dsn_train1$HasDetections)
      
      # Predict normal Logistic regression
      dsn_train1_p$p_o_1 <- predict(mylogit, newdata = dsn_train1, type="response")
      dsn_test1_p$p_o_1 <- predict(mylogit, newdata = dsn_test1, type="response")
      
      # Create a Logistic regression model based on the data created above to perform bagging
      for(i in 1:20){
        mylogit <- glm(HasDetections ~ ., data = dsn_train1_f[[i]], family = binomial)
        dsn_test1_p[ ,paste0("u_1_",i)] <- predict(mylogit, newdata = dsn_test1, type="response")
        dsn_train1_p[ ,paste0("u_1_",i)] <- predict(mylogit, newdata = dsn_train1, type="response")
      }
      
      # Take a vote of all the 20 predictions by averaging-out the predicted probabilities
      dsn_test1_p$p_1 <- rowMeans(dsn_test1_p[, grepl("^u",colnames(dsn_test1_p))])
      dsn_train1_p$p_1 <- rowMeans(dsn_train1_p[, grepl("^u",colnames(dsn_train1_p))])
      
      # Calculate AUC
      test1_auc <- as.numeric(sapply(colnames(dsn_test1_p)[-1], calculate_auc, dsn_test1_p))
      train1_auc <- as.numeric(sapply(colnames(dsn_train1_p)[-1], calculate_auc, dsn_train1_p))
      
      # Calculate other performance parameters like: Accuracy, Sensitivity, Specficity,
      # Top10Lift, Top20Lift, Top30Lift, Top40Lift, Top50Lift
      test1_perf_params <- lapply(colnames(dsn_test1_p)[-1], calculate_performance_params, dsn_test1_p)
      test1_perf_params <- bind_rows(test1_perf_params)
      
      train1_perf_params <- lapply(colnames(dsn_train1_p)[-1], calculate_performance_params, dsn_train1_p)
      train1_perf_params <- bind_rows(train1_perf_params)
      
      # Create resulting performance data frame
      dsn_auc <- data.frame("id" = colnames(dsn_test1_p)[-1],
                            "under_rate" = under_rate,
                            "test1_auc" = test1_auc,
                            "test1_acc" = test1_perf_params$acc,
                            "test1_sens" = test1_perf_params$Sensitivity,
                            "test1_spec" = test1_perf_params$Specificity,
                            "test1_Top10Lift" = test1_perf_params$Top10Lift,
                            "test1_Top20Lift" = test1_perf_params$Top20Lift,
                            "test1_Top30Lift" = test1_perf_params$Top30Lift,
                            "test1_Top40Lift" = test1_perf_params$Top40Lift,
                            "test1_Top50Lift" = test1_perf_params$Top50Lift,
                            "train1_auc" = train1_auc,
                            "train1_acc" = train1_perf_params$acc,
                            "train1_sens" = train1_perf_params$Sensitivity,
                            "train1_spec" = train1_perf_params$Specificity,
                            "train1_Top10Lift" = train1_perf_params$Top10Lift,
                            "train1_Top20Lift" = train1_perf_params$Top20Lift,
                            "train1_Top30Lift" = train1_perf_params$Top30Lift,
                            "train1_Top40Lift" = train1_perf_params$Top40Lift,
                            "train1_Top50Lift" = train1_perf_params$Top50Lift)
      
      # Creating file output path based on each CV and Run
      fileOutputPath <- create_output_file_path(cv_id,cvrun_id,under_rate)
      
      # Write the predictions and performance parameters separately
      write.csv(dsn_test1_p,file=fileOutputPath$filetest1PathPred,row.names = F)
      write.csv(dsn_train1_p,file=fileOutputPath$filetrain1PathPred,row.names = F)
      write.csv(dsn_auc,file=fileOutputPath$filePathAUC,row.names = F)
      
      end_time <- Sys.time()
      time_taken <- end_time - start_time
      print(paste0("Time Taken: ", time_taken))
      
      # Remove unwanted parameters from the environment
      rm(list=setdiff(ls(), c("create_file_path","create_output_file_path",
                              "random_under_sample_list_generator",
                              "random_bagging_list_generator",
                              "calculate_auc","under_rate",
                              "cv_id","cvrun_id","dsn_id","train1","test1",
                              "calculate_performance_params","calculate_lift")))
      # Garbage Collection
      gc() 
      gc()
    }
  }
}
